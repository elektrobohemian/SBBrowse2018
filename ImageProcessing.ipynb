{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723cdc7f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial is based on [this Jupyter notebook](DataProcessing.ipynb). If you haven't worked on it before, please visit it before continuing here.\n",
    "\n",
    "**Attention** This notebook cannot be run at this time. It is work in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410319be",
   "metadata": {},
   "source": [
    "### Exploring the Collection by Visual Content\n",
    "* reading the feature files takes approx. 12 min\n",
    "* erkl√§ren, woher die CBIR features kommen (https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)\n",
    "\n",
    "![Visual Words](img/visword.jpg)\n",
    "By Masterwaw - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=19645418\n",
    "\n",
    "Scale-invariant feature transform, reference for local feature detection\n",
    "\n",
    "![Visual Word Generation](img/visword_generation2.png)\n",
    "\n",
    "int sampleDocumentsToCreateCodebook = 5000; \n",
    "int numberOfClusters = 1000;\n",
    "\n",
    "next step takes ca. 40 minutes if the raw features are loaded from disk and converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20979d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureBaseDir=\"./featureFiles.5k1k/\"\n",
    "missingPPNs=[]\n",
    "readPPNs=[]\n",
    "featuresPPN=[]\n",
    "featsPerCentury=dict()\n",
    "readPpnPerCentury=dict()\n",
    "\n",
    "printLog(\"Loading features...\")\n",
    "if reinterpretVisualWordRawFeatures:\n",
    "    for century in range(7,21):\n",
    "        if century in grpCentury.groups:\n",
    "            featsPerCentury[century]=[]\n",
    "    for century in range(7,21):\n",
    "        if century in grpCentury.groups:\n",
    "            readPpnPerCentury[century]=[]\n",
    "\n",
    "    index=0\n",
    "    for row in df4.iterrows():\n",
    "        index=index+1\n",
    "        if index%10000==0:\n",
    "            printLog(\"Processed %i documents.\"%index)\n",
    "        ppn=str(row[1][\"PPN\"])\n",
    "        if os.path.isfile(featureBaseDir+ppn+\".csv\"):\n",
    "            #print ppn+\" okay.\"\n",
    "            featFile=open(featureBaseDir+ppn+\".csv\")\n",
    "            for line in featFile:\n",
    "                feature=line\n",
    "            tokens=feature.split()\n",
    "            harray=[]\n",
    "            for t in tokens:\n",
    "                harray.append(int(t,16))\n",
    "            featFile.close()\n",
    "\n",
    "            readPPNs.append(ppn)\n",
    "            featuresPPN.append(np.array(harray,dtype=np.uint8))\n",
    "            # check to which century the feature belongs\n",
    "            for century in range(7,21):\n",
    "                if century in grpCentury.groups:\n",
    "                    if ppn in ppnPerCentury[century]:\n",
    "                        readPpnPerCentury[century].append(ppn)\n",
    "                        featsPerCentury[century].append(np.array(harray,dtype=np.uint8))\n",
    "        else:\n",
    "            missingPPNs.append(ppn)\n",
    "    printLog(\"Done.\")\n",
    "    printLog(\"Number of missing PPNs: %i\"%len(missingPPNs))\n",
    "    \n",
    "    # pickling\n",
    "    pickleCompress('./picklez/missingPPNs.picklez',missingPPNs)\n",
    "    pickleCompress('./picklez/readPPNs.picklez',readPPNs)\n",
    "    pickleCompress('./picklez/featuresPPN.picklez',featuresPPN)\n",
    "    pickleCompress('./picklez/featsPerCentury.picklez',featsPerCentury)\n",
    "    pickleCompress('./picklez/readPpnPerCentury.picklez',readPpnPerCentury)\n",
    "    printLog(\"Pickling completed.\")\n",
    "else:\n",
    "    # takes about 1 minute\n",
    "    missingPPNs=pickleDecompress('./picklez/missingPPNs.picklez')\n",
    "    readPPNs=pickleDecompress('./picklez/readPPNs.picklez')\n",
    "    featuresPPN=pickleDecompress('./picklez/featuresPPN.picklez')\n",
    "    featsPerCentury=pickleDecompress('./picklez/featsPerCentury.picklez')\n",
    "    readPpnPerCentury=pickleDecompress('./picklez/readPpnPerCentury.picklez')\n",
    "    printLog(\"Loading from disk completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63ff12",
   "metadata": {},
   "source": [
    "* Clustering of 101031 elements started with 1000 as cluster target size: 3 min\n",
    "* note that the cluster labels vary for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with all features will most likely halt your computer because of the memory consumption if you use KMeans!\n",
    "feats=featuresPPN#[:20000] \n",
    "\n",
    "# define the number of clusters to be found\n",
    "true_k=1000\n",
    "printLog(\"Clustering of %i elements started with %i as cluster target size.\"%(len(feats),true_k))\n",
    "# initialize the k-means algorithm\n",
    "#km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "# we will rely on the mini batch k-means algorithm due to performance consideration otherwise your computer might crash...\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "# apply the algorithm on the data\n",
    "km.fit(feats)\n",
    "printLog(\"Clustering finished.\")\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d854b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir=\"./html/_clusteroverview_allcents/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "        \n",
    "imgDir=\"../../web/thumbnails/\"\n",
    "#imgDir=\"file:///Volumes/2TB_WD/sbb_images/tmp/\"\n",
    "htmlHead=\"<html><head></head><body bgcolor='#000000'>\"\n",
    "htmlTail=\"</body></html>\"\n",
    "clusters=dict()\n",
    "for i,val in enumerate(km.labels_):\n",
    "    if val not in clusters:\n",
    "        clusters[val]=[]\n",
    "    clusters[val].append(readPPNs[i])\n",
    "#print clusters\n",
    "for i in clusters:\n",
    "    htmlOut=open(saveDir+str(i)+\".html\",\"w\")\n",
    "    htmlOut.write(htmlHead+\"\\n\")\n",
    "    htmlOut.write(\"<a href='\"+str(i-1)+\".html'>last</a> &nbsp;\"+\"<a href='\"+str(i+1)+\".html'>next</a>\\n\"+\"<br />\")\n",
    "    for ppn in clusters[i]:\n",
    "        htmlOut.write(\"<img width='170' src='\"+imgDir+ppn+\".jpg' />\\n\")\n",
    "    htmlOut.write(htmlTail)\n",
    "    htmlOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeea4ab",
   "metadata": {},
   "source": [
    "repeat the same step for each century..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1535790",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersPerCentury=dict()\n",
    "maxClusterAmount=100\n",
    "printLog(\"Starting clustering per century...\")\n",
    "for century in featsPerCentury:\n",
    "    maxClusters=len(featsPerCentury[century])\n",
    "    # define the number of clusters to be found\n",
    "    true_k=int(maxClusters*0.1+1)\n",
    "    # restrict the number of clusters to prevent extremely large clusters\n",
    "    if true_k>(maxClusterAmount*2):\n",
    "        true_k=maxClusterAmount+int(maxClusters*0.005)\n",
    "    elif true_k>maxClusterAmount:\n",
    "        true_k=maxClusterAmount+int(maxClusters*0.01)\n",
    "    printLog(\"Clustering of %i element(s) started with %i as cluster target size for century %i.\"%(maxClusters,true_k,century))\n",
    "    # initialize the k-means algorithm\n",
    "    #km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    # we will rely on the mini batch k-means algorithm due to performance consideration otherwise your computer might crash...\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "    # apply the algorithm on the data\n",
    "    km.fit(featsPerCentury[century])\n",
    "    clustersPerCentury[century]=km.labels_\n",
    "printLog(\"Clustering finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f1fc9",
   "metadata": {},
   "source": [
    "Computing 'centroids' takes up to 6 minutes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCentroidsPerCentury=dict()\n",
    "numberCentroids=0\n",
    "\n",
    "printLog(\"Computing 'centroids' for...\")\n",
    "\n",
    "for century in clustersPerCentury:\n",
    "    printLog(\"\\tcentury %i\"%century)\n",
    "    clusters=dict()\n",
    "    centFeats=dict()\n",
    "\n",
    "    for i,val in enumerate(clustersPerCentury[century]):\n",
    "        if val not in centFeats:\n",
    "            centFeats[val]=[]\n",
    "        if val not in clusters:\n",
    "            clusters[val]=[]\n",
    "        index=readPPNs.index(readPpnPerCentury[century][i])\n",
    "        clusters[val].append(readPpnPerCentury[century][i])\n",
    "        centFeats[val].append(featuresPPN[index])\n",
    "\n",
    "    clusterCentroidsPerCentury[century]=dict()\n",
    "    for cluster in centFeats:\n",
    "        r=centFeats[cluster]\n",
    "        meanDistances=[]\n",
    "        D=pairwise_distances(r,r)\n",
    "        #A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None.\n",
    "        #If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y.\n",
    "        # distance between element 0 and 13 (=0.0 if X and Y are anti-correlated)\n",
    "        #D[0][13]\n",
    "        for row in D:\n",
    "            # each row in D stands for one document and its distances to all other documents\n",
    "            # by calculating its mean, we compute how dissimilar this document is to all others\n",
    "            meanDistances.append(np.mean(row))\n",
    "        #print meanDistances\n",
    "        minVal=np.min(meanDistances)\n",
    "        index=meanDistances.index(minVal)\n",
    "        clusterCentroidsPerCentury[century][cluster]=clusters[cluster][index]\n",
    "        numberCentroids=numberCentroids+1\n",
    "        #print str(cluster)+\": \"+str(clusters[cluster][index])+\" (of %i elements)\"%len(meanDistances)\n",
    "\n",
    "printLog(\"Done computing %i 'centroids'.\"%numberCentroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c2589",
   "metadata": {},
   "source": [
    "* iterate over all centuries and save output per centroids\n",
    "* CSV output is for the visualization with the web-based QA tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir=\"./html/_clusteroverview_per_century/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "        \n",
    "printLog(\"Creating HTML and CSV output...\")\n",
    "\n",
    "# limits the shown PPNs per century-separated cluster\n",
    "limitClusterCentroidsPerCentury=10\n",
    "# for the cluster detail views limitClusterCentroidsPerCentury*centuryLimitFactor elements will be displayed per centroid\n",
    "centuryLimitFactor=3\n",
    "\n",
    "csvOut=open(\"./web/data/clusters.csv\",\"w\")\n",
    "csvOut.write(\"id,value\\n\")\n",
    "rootNode=\"all.\"\n",
    "csvOut.write(\"all,\"+\"\\n\")\n",
    "\n",
    "for century in clustersPerCentury:\n",
    "    largestClusterSize=0\n",
    "    largestCluster=None\n",
    "    \n",
    "    csvCenturyOut=open(\"./web/data/\"+str(century)+\".csv\",\"w\")\n",
    "    csvCenturyOut.write(\"id,value\\n\")\n",
    "    rootCenturyNode=\"all\"\n",
    "    csvCenturyOut.write(rootCenturyNode+\",\"+\"\\n\")\n",
    "    \n",
    "    csvOut.write(\"all.\"+str(century)+\",\"+\"\\n\")\n",
    "    \n",
    "    saveDir=\"./html/_clusteroverview_per_century/\"+str(century)+\"/\"\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "    imgDir=\"../../../web/thumbnails/\"\n",
    "    #imgDir=\"file:///Volumes/2TB_WD/sbb_images/tmp/\"\n",
    "    htmlHead=\"<html><head></head><body bgcolor='#000000'>\"\n",
    "    htmlTail=\"</body></html>\"\n",
    "    \n",
    "    clusters=dict()\n",
    "    for i,val in enumerate(clustersPerCentury[century]):\n",
    "        if val not in clusters:\n",
    "            clusters[val]=[]\n",
    "        clusters[val].append(readPpnPerCentury[century][i])\n",
    "        \n",
    "\n",
    "    clusterSizes=[]\n",
    "    shownCentroidCount=0\n",
    "    shownMoreCentroidsAvailable=False\n",
    "    \n",
    "    noClustersInCentury=len(clusters)\n",
    "    for i in clusters:\n",
    "        clusterSizes.append(len(clusters[i]))\n",
    "        \n",
    "        if largestClusterSize<len(clusters[i]):\n",
    "            largestClusterSize=len(clusters[i])\n",
    "            largestCluster=i\n",
    "        \n",
    "        # the cluster's centroid\n",
    "        # 14/14: PPN789774356\n",
    "        #<br/>\n",
    "        #<img src='file:///Users/david/Documents/src/python/CulturalAnalytics/tmp/PPN789774356.jpg' />\n",
    "        #print \"\\tCentroid for cluster \"+str(i)+\": \"+str(clusterCentroidsPerCentury[century][i])\n",
    "        centroid=\"<img src='\"+imgDir+str(clusterCentroidsPerCentury[century][i])+\".jpg' />\\n\"+\"<br/>\\n\"\n",
    "    \n",
    "        if shownCentroidCount<limitClusterCentroidsPerCentury:\n",
    "            shownCentroidCount=shownCentroidCount+1\n",
    "            csvOut.write(rootNode+str(century)+\".\"+str(clusterCentroidsPerCentury[century][i])+\",\\n\")\n",
    "        else:\n",
    "            if not shownMoreCentroidsAvailable:\n",
    "                csvOut.write(rootNode+str(century)+\".more,\\n\")\n",
    "                shownMoreCentroidsAvailable=True\n",
    "                \n",
    "        csvCenturyOut.write(rootCenturyNode+\".\"+str(clusterCentroidsPerCentury[century][i])+\",\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        htmlOut=open(saveDir+str(i)+\".html\",\"w\")\n",
    "        htmlOut.write(htmlHead+\"\\n\")\n",
    "        #htmlOut.write(\"<a href='\"+str(century)+str(i-1)+\".html'>last</a> &nbsp;\"+\"<a href='\"+str(century)+str(i+1)+\".html'>next</a>\\n\"+\"<br />\")\n",
    "        htmlOut.write(\"<a href='\"+str(i-1)+\".html'>last</a> &nbsp;\"+\"<a href='\"+str(i+1)+\".html'>next</a>\\n\"+\"<br />\")\n",
    "        \n",
    "        htmlOut.write(centroid)\n",
    "        \n",
    "        centroidPPN=str(clusterCentroidsPerCentury[century][i])#str(clusters[i][0])\n",
    "        csvPPNOut=open(\"./web/data/\"+centroidPPN+\".csv\",\"w\")\n",
    "        csvPPNOut.write(\"id,value\\n\")\n",
    "        rootPPNNode=\"all\"\n",
    "        csvPPNOut.write(rootPPNNode+\",\"+\"\\n\")\n",
    "        csvPPNOut.write(rootPPNNode+\".\"+centroidPPN+\",\\n\")\n",
    "        ppnCount=0\n",
    "    \n",
    "        for ppn in clusters[i]:\n",
    "            htmlOut.write(\"<img width='170' src='\"+imgDir+ppn+\".jpg' alt='\"+ppn+\"'/>\\n\")\n",
    "            #csvOut.write(rootNode+str(century)+\".\"+str(clusterCentroidsPerCentury[century][i])+\".\"+ppn+\",\\n\")\n",
    "            if ppnCount<limitClusterCentroidsPerCentury*centuryLimitFactor:\n",
    "                ppnCount=ppnCount+1\n",
    "                csvCenturyOut.write(rootCenturyNode+\".\"+str(clusterCentroidsPerCentury[century][i])+\".\"+ppn+\",\\n\")\n",
    "            else:\n",
    "                csvCenturyOut.write(rootCenturyNode+\".\"+str(clusterCentroidsPerCentury[century][i])+\".more,\\n\")\n",
    "                break\n",
    "            \n",
    "            csvPPNOut.write(rootPPNNode+\".\"+centroidPPN+\".\"+ppn+\",\\n\")\n",
    "        csvPPNOut.close()\n",
    "        htmlOut.write(htmlTail)\n",
    "        htmlOut.close()\n",
    "    \n",
    "    csvCenturyOut.close()\n",
    "    print(\"\\tLargest cluster for century %i is %i with %i elements.\"%(century,largestCluster,largestClusterSize))\n",
    "    print(\"\\t\\tNumber of clusters: %i\"%noClustersInCentury)\n",
    "    print(\"\\t\\tMean cluster size: %s\"%str(np.mean(clusterSizes)))\n",
    "    print(\"\\t\\tCluster size standard deviation: %s\"%str(np.std(clusterSizes)))\n",
    "    print(\"\\t\\tMin. cluster size: %s\"%str(np.amin(clusterSizes)))\n",
    "    print(\"\\t\\tMax. cluster size: %s\"%str(np.amax(clusterSizes)))\n",
    "csvOut.close()\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a11c8",
   "metadata": {},
   "source": [
    "### A Visual Timeline of Publications\n",
    "plotting of the \"centroids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37bc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDir=\"../web/thumbnails/\"\n",
    "htmlHead=\"<html><head></head><body bgcolor='#000000'>\"\n",
    "htmlTail=\"</body></html>\"\n",
    "centroidPath=\"html/_centroids.html\"\n",
    "\n",
    "printLog(\"Saving centroid overview HTML page at: \"+centroidPath)\n",
    "htmlOut=open(centroidPath,\"w\")\n",
    "htmlOut.write(htmlHead)\n",
    "for century in clustersPerCentury:\n",
    "    htmlOut.write(\"<h1 style='color:white;'>\"+str(century)+\"</h1>\\n\")\n",
    "    for centroid in clusterCentroidsPerCentury[century]:\n",
    "        htmlOut.write(\"<img width='170' src='\"+imgDir+clusterCentroidsPerCentury[century][centroid]+\".jpg' />\\n\")\n",
    "htmlOut.write(htmlTail)\n",
    "htmlOut.close()\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de485ee4",
   "metadata": {},
   "source": [
    "we can also extend the idea and create a graph of the data\n",
    "to base a nice visualization of the clusters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e596e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDir=\"./web/thumbnails/\"\n",
    "printLog(\"Creating overview graph...\")\n",
    "G=nx.Graph()\n",
    "\n",
    "lastCentury=\"7\"\n",
    "for century in clustersPerCentury: \n",
    "#for century in [7,10,11,12,13,14,15,16]:#range(12,15):\n",
    "    strCentury=str(century)\n",
    "    G.add_node(strCentury)\n",
    "    G.node[strCentury]['name'] = strCentury\n",
    "    G.node[strCentury]['type'] = \"century\"\n",
    "    \n",
    "    for centroid in clusterCentroidsPerCentury[century]:\n",
    "        ppn=str(clusterCentroidsPerCentury[century][centroid])\n",
    "        imagePath=imgDir.replace(\"file://\",\"\")+ppn+\".jpg\"\n",
    "        G.add_node(ppn)\n",
    "        G.node[ppn]['name'] = ppn\n",
    "        if os.path.isfile(imagePath):\n",
    "            G.node[ppn]['name'] = ppn\n",
    "            G.node[ppn]['imagePath'] = ppn\n",
    "        else:\n",
    "            G.node[ppn]['name'] = ppn\n",
    "            G.node[ppn]['imagePath'] = \"none\"\n",
    "        G.node[ppn]['title'] = ppnLookup[ppn]['title']\n",
    "        G.node[ppn]['creator'] = ppnLookup[ppn]['creator']\n",
    "        if ppnLookup[ppn]['spatialClusterName']:\n",
    "            G.node[ppn]['location'] = ppnLookup[ppn]['spatialClusterName']\n",
    "        G.node[ppn]['locationRaw'] =ppnLookup[ppn]['spatialRaw']\n",
    "        \n",
    "        G.node[ppn]['mediatype'] =ppnLookup[ppn]['mediatype']\n",
    "        G.node[ppn]['subject'] =ppnLookup[ppn]['subject']\n",
    "        G.node[ppn]['source'] =ppnLookup[ppn]['source']\n",
    "        G.node[ppn]['publisher'] =ppnLookup[ppn]['publisher']\n",
    "        G.node[ppn]['alternative'] =ppnLookup[ppn]['alternative']\n",
    "            \n",
    "        G.node[ppn][\"century\"]=century\n",
    "        G.node[ppn]['dateClean'] =ppnLookup[ppn]['dateClean']\n",
    "        G.node[ppn][\"cluster\"]=str(centroid)\n",
    "        G.node[ppn]['lat']=ppnLookup[ppn]['lat']\n",
    "        G.node[ppn]['lng']=ppnLookup[ppn]['lng']\n",
    "        G.node[ppn]['type'] = \"image\"\n",
    "        G.node[ppn]['textCluster'] =ppnLookup[ppn]['textCluster']\n",
    "        G.node[ppn]['creatorCluster'] =ppnLookup[ppn]['creatorCluster']\n",
    "        \n",
    "        G.add_edge(strCentury,ppn)\n",
    "    G.add_edge(strCentury,lastCentury)\n",
    "    lastCentury=strCentury\n",
    "    \n",
    "nx.write_gml(G,\"graphs/century.gml\")\n",
    "d = json_graph.node_link_data(G)\n",
    "jsonPath='./web/data/century.json'\n",
    "#jsonPath='./force/century_test.json'\n",
    "saveDir=\"./web/force/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "        \n",
    "json.dump(d, open(jsonPath,'w'))\n",
    "printLog(\"Done (see %s).\"%jsonPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82838261",
   "metadata": {},
   "outputs": [],
   "source": [
    "printLog(\"Creating cluster graph output...\")\n",
    "imgDir=\"./web/thumbnails/\"\n",
    "\n",
    "saveDir=\"./web/data/clusters/\"\n",
    "if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "for century in clustersPerCentury:\n",
    "    printLog(\"Processing century \"+str(century))\n",
    "    saveDir=\"./web/data/clusters/\"+str(century)+\"/\"\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "    clusters=dict()\n",
    "    for i,val in enumerate(clustersPerCentury[century]):\n",
    "        if val not in clusters:\n",
    "            clusters[val]=[]\n",
    "        clusters[val].append(readPpnPerCentury[century][i])\n",
    "\n",
    "    for i in clusters:\n",
    "        G=nx.Graph()\n",
    "        # the cluster's centroid\n",
    "        centroid=str(clusterCentroidsPerCentury[century][i])\n",
    "        G.add_node(centroid)\n",
    "        G.node[centroid]['type'] = \"centroid\"\n",
    "        \n",
    "        for ppn in clusters[i]:\n",
    "            imagePath=imgDir.replace(\"file://\",\"\")+ppn+\".jpg\"\n",
    "            #dateClean=str(df4[df4.PPN==ppn].iloc[-1]['dateClean'])\n",
    "            dateClean=ppnLookup[ppn][\"dateClean\"]\n",
    "            G.add_node(dateClean)\n",
    "            G.node[dateClean]['name'] = dateClean\n",
    "            G.node[dateClean]['type'] = \"dateClean\"\n",
    "            G.add_edge(centroid,dateClean)\n",
    "            \n",
    "            G.add_node(ppn)\n",
    "            G.node[ppn]['name'] = ppn\n",
    "            if os.path.isfile(imagePath):\n",
    "                G.node[ppn]['name'] = ppn\n",
    "                G.node[ppn]['imagePath'] = ppn\n",
    "            else:\n",
    "                G.node[ppn]['name'] = ppn\n",
    "                G.node[ppn]['imagePath'] = \"none\"\n",
    "            #G.node[ppn]['title'] = df4[df4.PPN==ppn].iloc[-1]['title']\n",
    "            #G.node[ppn]['creator'] = str(df4[df4.PPN==ppn].iloc[-1]['creator'])\n",
    "            #G.node[ppn]['location'] = df4[df4.PPN==ppn].iloc[-1]['spatialClusterName']\n",
    "            G.node[ppn]['title'] = ppnLookup[ppn][\"title\"]\n",
    "            G.node[ppn]['creator'] = ppnLookup[ppn][\"creator\"]\n",
    "            if ppnLookup[ppn][\"spatialClusterName\"]:\n",
    "                G.node[ppn]['location'] = ppnLookup[ppn][\"spatialClusterName\"]\n",
    "            else:\n",
    "                G.node[ppn]['location']=\"none\"\n",
    "            G.node[ppn]['locationRaw'] =ppnLookup[ppn]['spatialRaw']\n",
    "            \n",
    "            G.node[ppn]['mediatype'] =ppnLookup[ppn]['mediatype']\n",
    "            G.node[ppn]['subject'] =ppnLookup[ppn]['subject']\n",
    "            G.node[ppn]['source'] =ppnLookup[ppn]['source']\n",
    "            G.node[ppn]['publisher'] =ppnLookup[ppn]['publisher']\n",
    "            G.node[ppn]['alternative'] =ppnLookup[ppn]['alternative']\n",
    "            G.node[ppn]['dateClean'] =ppnLookup[ppn]['dateClean']\n",
    "            G.node[ppn]['lat']=ppnLookup[ppn]['lat']\n",
    "            G.node[ppn]['lng']=ppnLookup[ppn]['lng']\n",
    "            \n",
    "            if ppn==centroid:\n",
    "                G.node[centroid]['type'] = \"centroid\"\n",
    "            else:\n",
    "                G.node[ppn]['type'] = \"image\"\n",
    "            \n",
    "            G.add_edge(dateClean,ppn)\n",
    "            \n",
    "        nx.write_gml(G,saveDir+str(i)+\".gml\")\n",
    "        d = json_graph.node_link_data(G)\n",
    "        jsonPath='./web/data/clusters/'+str(century)+'/'+str(i)+'.json'\n",
    "        json.dump(d, open(jsonPath,'w'))\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c8aa7",
   "metadata": {},
   "source": [
    "## Geospatial Extravaganza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points are latitude, longitude\n",
    "# Latitudes range from -90 to 90.\n",
    "# Longitudes range from -180 to 180\n",
    "newport_ri = (41.49008, -71.312796)\n",
    "cleveland_oh = (41.499498, -81.695391)\n",
    "test=(-95,-161)\n",
    "x=geodesic(newport_ri, test)\n",
    "print(x.kilometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "ppnPosition=[]\n",
    "ppnPositionLabels=[]\n",
    "ppnPositionSpatialClean=[]\n",
    "\n",
    "printLog(\"Extracting latitude and longitude...\")\n",
    "\n",
    "for row in df4.iterrows():\n",
    "    index=index+1\n",
    "    if index%10000==0:\n",
    "        printLog(\"Processed %i documents.\"%index)\n",
    "    ppn=str(row[1][\"PPN\"])\n",
    "    try:\n",
    "        lat=float(row[1][\"latitude\"])\n",
    "        lng=float(row[1][\"longitude\"])\n",
    "        spatialClean=row[1][\"spatialClean\"]\n",
    "        if math.isnan(lat) and math.isnan(lng):\n",
    "            pass\n",
    "        else:\n",
    "            ppnPositionLabels.append(ppn)\n",
    "            ppnPositionSpatialClean.append(spatialClean)\n",
    "            ppnPosition.append((lat,lng))\n",
    "    except TypeError:\n",
    "        #print(row[1][\"latitude\"])\n",
    "        pass\n",
    "    \n",
    "printLog(\"Found %i PPNs with coordinates.\" %len(ppnPositionLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Distribution of Geo-Spatial Coordinates')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.scatter(*zip(*ppnPosition),alpha=0.1)\n",
    "if saveFiguresAsPDF:\n",
    "    plt.savefig('./figures/sample.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240fb10",
   "metadata": {},
   "source": [
    "* Latitudes range from -90 to 90. -> x-Achse sollte 180 breit sein, Mitte ist 90\n",
    "* Longitudes range from -180 to 180 -> y-Achse sollte 360 breit sein, Mitte ist 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=ppnPosition\n",
    "\n",
    "# define the number of clusters to be found\n",
    "true_k=30\n",
    "printLog(\"Clustering of %i elements started with %i as cluster target size.\"%(len(feats),true_k))\n",
    "# initialize the k-means algorithm\n",
    "#km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "# we will rely on the mini batch k-means algorithm due to performance consideration otherwise your computer might crash...\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "# apply the algorithm on the data\n",
    "km.fit(feats)\n",
    "printLog(\"Clustering finished.\")\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbe2ff",
   "metadata": {},
   "source": [
    "bounding boxes aus http://boundingbox.klokantech.com/\n",
    "\n",
    "Visualization with GeoJSON (http://geojson.org/) https://pypi.python.org/pypi/geojson and OpenLayers  http://openlayers.org/en/latest/doc/ http://openlayers.org/en/latest/apidoc/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions=[\"Europe\",\"Africa\",\"Asia\",\"Australia\",\"SouthAmerica\",\"NorthAmerica\"]\n",
    "regionBoundingBox=dict()\n",
    "#westlimit=-22.5; southlimit=33.6; eastlimit=58.4; northlimit=82.9\n",
    "regionBoundingBox[\"Europe\"]=[33.6,82.9,-22.5,58.4]\n",
    "#africa westlimit=-22.9; southlimit=-63.7; eastlimit=58.0; northlimit=37.2\n",
    "regionBoundingBox[\"Africa\"]=[-63.7,37.2,-22.9,58.0]\n",
    "#asia westlimit=29.5; southlimit=-11.4; eastlimit=-168.4; northlimit=81.2\n",
    "regionBoundingBox[\"Asia\"]=[-11.4,81.2,-168.4,29.5]\n",
    "#australia westlimit=112.5; southlimit=-50.4; eastlimit=-162.9; northlimit=-10.0\n",
    "regionBoundingBox[\"Australia\"]=[-50.4,-10.0,-162.9,112.5]\n",
    "#south america westlimit=-119.5; southlimit=-57.0; eastlimit=-29.9; northlimit=28.1\n",
    "regionBoundingBox[\"SouthAmerica\"]=[-57.0,28.1,-119.5,-29.9]\n",
    "#north america westlimit=-169.1; southlimit=23.7; eastlimit=-20.0; northlimit=71.4\n",
    "regionBoundingBox[\"NorthAmerica\"]=[23.7,71.4,-169.1,-20.0]\n",
    "\n",
    "\n",
    "# man muss checken in welchen intervallen die kontinente liegen, insb. bei 2 und 3\n",
    "def getRegion(lat,lng):\n",
    "    for region in regions:\n",
    "        if region in regionBoundingBox:\n",
    "            if regionBoundingBox[region][0] <= lat <= regionBoundingBox[region][1]:\n",
    "                if regionBoundingBox[region][2] <= lng <= regionBoundingBox[region][3]:\n",
    "                    return region\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "#print getRegion(51,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd144b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all arrays and check if there is more than one location associated with a PPN\n",
    "# ppnPosition is (lat,lng)\n",
    "printLog(\"Creating GeoJSON data...\")\n",
    "multipleCitiesPPN=pickleDecompress(\"./picklez/multipleSpatialNamesPerPPN.picklez\")\n",
    "\n",
    "baseDir=\"./web/data/layers/\"\n",
    "fileName=baseDir+\"test.json\"\n",
    "locationFeatures=dict()\n",
    "locationFeaturesGlobal=[]\n",
    "\n",
    "spots=zip(ppnPositionLabels, ppnPositionSpatialClean,ppnPosition)\n",
    "\n",
    "for spot in spots:\n",
    "    ppn=spot[0]\n",
    "    leadingLoc=spot[1]\n",
    "    latLng=spot[2]\n",
    "    # GeoJSON points are in longitude , latitude but our storage is lat/lng\n",
    "    my_point = gj.Point((latLng[1], latLng[0]))\n",
    "\n",
    "    region=getRegion(latLng[0],latLng[1])\n",
    "    if region not in locationFeatures:\n",
    "        locationFeatures[region]=[]\n",
    "        print(\"Adding \"+region)\n",
    "    else:\n",
    "        locationFeatures[region].append(gj.Feature(geometry=my_point, properties={\"title\": leadingLoc+\" \"+ppn, \"ppn\":ppn}))\n",
    "    \n",
    "    locationFeaturesGlobal.append(gj.Feature(geometry=my_point, properties={\"title\": leadingLoc+\" \"+ppn, \"ppn\":ppn}))\n",
    "    #print ppn\n",
    "    if ppn in multipleCitiesPPN:\n",
    "        array=multipleCitiesPPN[ppn]\n",
    "        #if there are alternative locations for this PPN\n",
    "        if len(array)>1:\n",
    "            #print leadingLoc+\" \"+ppn\n",
    "            #print \"\\t\"+\"; \".join(array).encode(\"utf-8\")\n",
    "            pass\n",
    "\n",
    "for region in locationFeatures:\n",
    "    geoJSON_collection=gj.FeatureCollection(locationFeatures[region])\n",
    "    dump = gj.dumps(geoJSON_collection, sort_keys=True)\n",
    "\n",
    "    jsonFile = open(baseDir+region.lower()+\".json\", \"w\")\n",
    "    jsonFile.write(dump)\n",
    "    jsonFile.close()\n",
    "    \n",
    "\n",
    "geoJSON_collection=gj.FeatureCollection(locationFeaturesGlobal)\n",
    "dump = gj.dumps(geoJSON_collection, sort_keys=True)\n",
    "\n",
    "globalJSONPath=baseDir+\"global.json\"\n",
    "jsonFile = open(globalJSONPath, \"w\")\n",
    "jsonFile.write(dump)\n",
    "jsonFile.close()\n",
    "printLog(\"Serialized %i metadata records.\\n\\tSaved global JSON document at %s.\" %(len(locationFeaturesGlobal),globalJSONPath))\n",
    "\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b51cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print spots.index(\"PPN766441857\")\n",
    "#print(spots[10])\n",
    "#print(getRegion(spots[10][2][0],spots[10][2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create an inverted mapping from alternative location names to \"leading\" location names\n",
    "# the resulting dicts \"main\" key will be the leading character of the alternative location name \n",
    "# within this map will be a mapping to the \"leading\" location name, e.g., 'Corfu' would be mapped to 'Korfu'\n",
    "# lastLetter=r[0].lower()\n",
    "osmNames=pickleDecompress('./picklez/osm_names.picklez')\n",
    "osmAlternativesSorted=dict()\n",
    "for leadLoc in osmNames:\n",
    "    #print leadLoc\n",
    "    for v in osmNames[leadLoc].values():\n",
    "        leadingLetter=v[0].lower()\n",
    "        if not leadingLetter in osmAlternativesSorted:\n",
    "            osmAlternativesSorted[leadingLetter]=dict()\n",
    "        else:\n",
    "            osmAlternativesSorted[leadingLetter][v]=leadLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# osmNames(key): alle alternativen titel in der Form names[loc][u'name']:\n",
    "# u'Ems': {u'name:nl': u'Eems', u'name': u'Ems', u'name:de': u'Ems', u'name:la': u'Amisia'...\n",
    "# latLng(key)[lat|lng]: latitude/longitude pro key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672953f",
   "metadata": {},
   "source": [
    "# Service Functions\n",
    "to avoid problems with JSON access you should load the web pages from your own HTTP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a72b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if launchHTTPServer:\n",
    "    # the resulting HTTP service will listen on port 8000 and open the main page in the browser\n",
    "    import http_server\n",
    "    http_server.load_url('web/webapps/index.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
